{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76828af-0e06-4d7f-a13f-178e782b731b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Pipeline Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       1.00      1.00      1.00         4\n",
      "    phishing       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           1.00         7\n",
      "   macro avg       1.00      1.00      1.00         7\n",
      "weighted avg       1.00      1.00      1.00         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "data = pd.read_csv(\"emails.csv\")\n",
    "texts = data['text'].tolist()\n",
    "labels = data['label']\n",
    "label_map = {'legitimate': 0, 'phishing': 1}\n",
    "numeric_labels = [label_map[l] for l in labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, numeric_labels, test_size=0.5, random_state=42, stratify=numeric_labels)\n",
    "\n",
    "# Train baseline model\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_counts, y_train)\n",
    "\n",
    "# Prepare LLM embeddings\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "X_train_emb = get_embeddings(X_train)\n",
    "X_test_emb = get_embeddings(X_test)\n",
    "\n",
    "llm_clf = LogisticRegression()\n",
    "llm_clf.fit(X_train_emb, y_train)\n",
    "\n",
    "# Combined decision\n",
    "nb_probs = nb_model.predict_proba(X_test_counts)\n",
    "llm_predictions = llm_clf.predict(X_test_emb)\n",
    "\n",
    "final_predictions = []\n",
    "for i, probs in enumerate(nb_probs):\n",
    "    phishing_prob = probs[1]  # Probability of phishing\n",
    "    if phishing_prob > 0.9:\n",
    "        final_predictions.append(1)\n",
    "    elif phishing_prob < 0.1:\n",
    "        final_predictions.append(0)\n",
    "    else:\n",
    "        final_predictions.append(llm_predictions[i])\n",
    "\n",
    "y_pred = final_predictions  # Use final_predictions as y_pred\n",
    "\n",
    "print(\"Combined Pipeline Results:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"legitimate\", \"phishing\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4ca719-cc02-4047-a990-8f5471a8a67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Pipeline Results:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  legitimate       0.98      0.99      0.99      2202\n",
      "    phishing       0.99      0.96      0.98      1200\n",
      "\n",
      "    accuracy                           0.98      3402\n",
      "   macro avg       0.98      0.98      0.98      3402\n",
      "weighted avg       0.98      0.98      0.98      3402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch, numpy as np\n",
    "\n",
    "# ------------------ 1. Load data -----------------\n",
    "df = pd.read_csv(\"emails_from_spamassassin.csv\")\n",
    "texts  = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].map({\"legitimate\":0, \"phishing\":1}).tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.5, random_state=42, stratify=labels)\n",
    "\n",
    "# ------------------ 2. Baseline NB ---------------\n",
    "vectorizer = CountVectorizer()\n",
    "Xtr_counts = vectorizer.fit_transform(X_train)\n",
    "Xte_counts = vectorizer.transform(X_test)\n",
    "\n",
    "nb_model = MultinomialNB().fit(Xtr_counts, y_train)\n",
    "\n",
    "# ------------------ 3. LLM setup -----------------\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer   = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_model   = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "llm_model.to(device)\n",
    "\n",
    "def embed(batch_texts, batch_size=32, max_len=256):\n",
    "    \"\"\"Return (N, 768) numpy matrix of mean-pooled embeddings.\"\"\"\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(batch_texts), batch_size):\n",
    "        batch = batch_texts[i:i+batch_size]\n",
    "        tok = tokenizer(batch,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=max_len).to(device)\n",
    "        with torch.no_grad():\n",
    "            hidden = llm_model(**tok).last_hidden_state  # (b, seq, 768)\n",
    "            vecs   = hidden.mean(dim=1).cpu()            # (b, 768)\n",
    "        all_vecs.append(vecs)\n",
    "    return torch.cat(all_vecs, dim=0).numpy()\n",
    "\n",
    "X_train_emb = embed(X_train, batch_size=32)\n",
    "X_test_emb  = embed(X_test,  batch_size=32)\n",
    "\n",
    "llm_clf = LogisticRegression(max_iter=1000).fit(X_train_emb, y_train)\n",
    "\n",
    "# ------------------ 4. Two-stage decision --------\n",
    "nb_probs       = nb_model.predict_proba(Xte_counts)\n",
    "llm_pred       = llm_clf.predict(X_test_emb)\n",
    "final_pred = []\n",
    "\n",
    "for prob, fallback in zip(nb_probs, llm_pred):\n",
    "    p = prob[1]              # phishing probability\n",
    "    if p > 0.9:   final_pred.append(1)\n",
    "    elif p < 0.1: final_pred.append(0)\n",
    "    else:         final_pred.append(fallback)\n",
    "\n",
    "print(\"Combined Pipeline Results:\\n\")\n",
    "print(classification_report(y_test, final_pred,\n",
    "                            target_names=[\"legitimate\", \"phishing\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15baf92-2bf3-466a-9793-779faafa8b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
